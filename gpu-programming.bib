
@INPROCEEDINGS{fialka06,
  author       = {Fialka, O. and Cadik, M.},
  booktitle    = {10th Int. Conference on Information Visualization},
  title        = {{FFT} and Convolution Performance in Image Filtering on {GPU}},
  year         = {2006},
  month        = {july},
  pages        = {609--614},
  doi          = {10.1109/IV.2006.53},
  issn         = {1550-6037},
  address={London},
  publisher={IEEE},
}

@ARTICLE{Jacques2010285,
  title        = {Towards real-time radiation therapy: GPU accelerated superposition/convolution},
  journal      = {Computer Methods and Programs in Biomedicine},
  volume       = {98},
  number       = {3},
  pages        = {285 - 292},
  year         = {2010},
  issn         = {0169-2607},
  doi          = {10.1016/j.cmpb.2009.07.004},
  author       = {Robert Jacques and Russell Taylor and John Wong and Todd McNutt},
}

@INCOLLECTION{convo01,
  author       = {Sobolev, V.I.},
  editor       = {Hazewinkel, M.},
  title        = {Convolution of functions},
  booktitle    = {Encyclopedia of Mathematics},
  year         = {2001},
  publisher    = {Springer-Verlag},
}

@inproceedings{ykman2010exploration,
 title={Exploration framework for run-time resource management of embedded multi-core platforms},
 author={Ykman-Couvreur, C.},
 booktitle={Embedded Computer Systems (SAMOS), 2010 International Conference on},
 pages={333--340},
 year={2010},
 organization={IEEE}
}

@article{ykman2007design,
 title={Design-time application mapping and platform exploration for MP-SoC customised run-time management},
 author={Ykman-Couvreur, C. and Nollet, V. and Marescaux, T. and Brockmeyer, E. and Catthoor, F. and Corporaal, H.},
 journal={Computers \& Digital Techniques, IET},
 volume={1},
 number={2},
 pages={120--128},
 year={2007},
 publisher={IET}
}

@inproceedings{avasare2010practical,
 title={Practical approach for design space explorations using simulators at multiple abstraction levels},
 author={Avasare, P. and Vanmeerbeeck, G. and Kavka, C. and Mariani, G.},
 booktitle={Proc. Design Automation Conf. User Track, Anaheim, CA},
 year={2010}
}

@article{ykman2011linking,
 title={Linking run-time resource management of embedded multi-core platforms with automated design-time exploration},
 author={Ykman-Couvreur, C. and Avasare, P. and Mariani, G. and Palermo, G. and Silvano, C. and Zaccaria, V.},
 journal={Computers \& Digital Techniques, IET},
 volume={5},
 number={2},
 pages={123--135},
 year={2011},
 publisher={IET}
}

@article{browne2012survey,
 title={A survey of monte carlo tree search methods},
 author={Browne, C.B. and Powley, E. and Whitehouse, D. and Lucas, S.M. and Cowling, P.I. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
 journal={Computational Intelligence and AI in Games, IEEE Transactions on},
 volume={4},
 number={1},
 pages={1--43},
 year={2012},
 publisher={IEEE}
}

@article{darlington77,
 author        = {R. M. Burstall and John Darlington},
 title         = {A {T}ransformation {S}ystem for {D}eveloping {R}ecursive {P}rograms},
 journal       = {J. ACM},
 volume        = {24},
 number        = {1},
 year          = {1977},
 issn          = {0004-5411},
 pages         = {44--67},
 doi           = {http://doi.acm.org/10.1145/321992.321996},
 publisher     = {ACM Press},
 address       = {New York, NY, USA},
 }

@inproceedings{skepu,
 author = {Enmyren, Johan and Kessler, Christoph W.},
 title = {SkePU: a multi-backend skeleton programming library for multi-GPU systems},
 booktitle = {Proceedings of the fourth international workshop on High-level parallel programming and applications},
 series = {HLPP '10},
 year = {2010},
 isbn = {978-1-4503-0254-8},
 location = {Baltimore, Maryland, USA},
 pages = {5--14},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1863482.1863487},
 doi = {10.1145/1863482.1863487},
 acmid = {1863487},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cuda, data parallelism, gpu, opencl, skeleton programming},
}

@inproceedings{nakhost2009monte,
 title={Monte-Carlo exploration for deterministic planning},
 author={Nakhost, H. and M{\"u}ller, M.},
 booktitle={IJCAI},
 pages={1766--1771},
 year={2009}
}

@article{pellier2010uct,
 title={An UCT Approach for Anytime Agent-Based Planning},
 author={Pellier, D. and Bouzy, B. and M{\'e}tivier, M.},
 journal={Advances in Practical Applications of Agents and Multiagent Systems},
 pages={211--220},
 year={2010},
 publisher={Springer}
}

@inproceedings{chaslot2006monte,
 title={Monte-Carlo tree search in production management problems},
 author={Chaslot, G. and De Jong, S. and Saito, J.T. and Uiterwijk, J.},
 booktitle={Proceedings of the 18th BeNeLux Conference on Artificial Intelligence},
 pages={91--98},
 year={2006}
}

@article{couetoux2011continuous,
 title={Continuous upper confidence trees},
 author={Cou{\"e}toux, A. and Hoock, J.B. and Sokolovska, N. and Teytaud, O. and Bonnard, N.},
 journal={Learning and Intelligent Optimization},
 pages={433--445},
 year={2011},
 publisher={Springer}
}

@inproceedings{walsh2010integrating,
 title={Integrating sample-based planning and model-based reinforcement learning},
 author={Walsh, T.J. and Goschin, S. and Littman, M.L.},
 booktitle={Proceedings of AAAI},
 number={1},
 year={2010}
}



@inproceedings{FFTTuning,
 author = {Dotsenko, Yuri and Baghsorkhi, Sara S. and Lloyd, Brandon and Govindaraju, Naga K.},
 title = {{Auto-Tuning of Fast Fourier Transform on Graphics Processors}},
 booktitle = {{Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming}},
 series = {PPoPP '11},
 year = {2011},
 isbn = {978-1-4503-0119-0},
 location = {San Antonio, TX, USA},
 pages = {257--266},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1941553.1941589},
 doi = {10.1145/1941553.1941589},
 acmid = {1941589},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {auto-tuning, fast fourier transform, fft, gpu, high performance, performance analysis, performance tuning},
 whatIsThis = {they present a techinque for automatically generating efficient kernels 
for specific instances of the FFT problem. basically, they have a set of parameters of the FFT problem that needs to be solved
(size of the data, singe or double precision, real of complex numbers), and a set of parameters for the desired kernel (single instance
or streaming FFT, coalesced or uncoalesced I/O etc.), and based on this they generate the best kernel, division of the data, number of
blocks and threads and so on. it is not very clear what method for generating kernel based on parameters they use, but i don't think they
are using machine learning. it looks like they use just parameter sweep with some ad-hoc heuristics for prunning the search space. good
for related work.}
} 



@inproceedings{WarpCentric,
 author = {Hong, Sungpack and Kim, Sang Kyun and Oguntebi, Tayo and Olukotun, Kunle},
 title = {{Accelerating CUDA Graph Algorithms at Maximum Warp}},
 booktitle = {Proceedings of the 16th ACM symposium on Principles and practice of parallel programming},
 series = {PPoPP '11},
 year = {2011},
 isbn = {978-1-4503-0119-0},
 location = {San Antonio, TX, USA},
 pages = {267--276},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1941553.1941590},
 doi = {10.1145/1941553.1941590},
 acmid = {1941590},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cuda, gpgpu, parallel graph algorithms},
 whatIsThis = {quite interesing paper, where they present a (novel?) approach to programming GPU, in order to avoid branch divergence within the
threads from the same warp and scattering memory accesses. basically, they introduce "warp-centric" method of programming, where the basic of unit
is a virtual warp, rather than a thread. all threads in a virtual warp progress in SISD manner, and all of the SIMD parts of the virtual warp are 
explicitly annotated. so, if you have threads that diverge, place them into different virtual warps, even if that means that 
virtual warps are much smaller that the actual physical warps. at one extreme, you can have one thread per warp. in order to improve performance, you
can group several small virtual warps into a larger physical warp, which introduces divergance, but possibly better utilizes GPU cores. they demonstrate
their approach on graph alorithms that operate on irregular graphs, and show speedups of up to 16 over sequential CPU implementations. additionally, their
GPU code outperforms SMP CPU code. to conclude, probably not that relevant to us, but still somewhat interesting.
}
}

@inproceedings{MultiGPUsOpenCL,
 author = {Kim, Jungwon and Kim, Honggyu and Lee, Joo Hwan and Lee, Jaejin},
 title = {{Achieving a Single Compute Device Image in OpenCL for Multiple GPUs}},
 booktitle = {Proceedings of the 16th ACM symposium on Principles and practice of parallel programming},
 series = {PPoPP '11},
 year = {2011},
 isbn = {978-1-4503-0119-0},
 location = {San Antonio, TX, USA},
 pages = {277--288},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1941553.1941591},
 doi = {10.1145/1941553.1941591},
 acmid = {1941591},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {access range analysis, compilers, opencl, runtime, virtual device memory, workload distribution},
 whatIsThis = {extension to OpenCL which enables seeing multiple GPUs as one single GPU. they claim that they have very good performance
with their thing on systems comprising 8 GPUs. fair enough. maybe worth reading sometimes in the future.}
}

@inproceedings{MultiGPUStreamIt,
 author = {Huynh, Huynh Phung and Hagiescu, Andrei and Wong, Weng-Fai and Goh, Rick Siow Mong},
 title = {{Scalable Framework for Mapping Streaming Applications onto Multi-GPU Systems}},
 booktitle = {Proceedings of the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming},
 series = {PPoPP '12},
 year = {2012},
 isbn = {978-1-4503-1160-1},
 location = {New Orleans, Louisiana, USA},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2145816.2145818},
 doi = {10.1145/2145816.2145818},
 acmid = {2145818},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-GPU, scalable, streaming, streamit},
 whatIsThis = {They present the extension of something called StreamIt for systems with multiple GPUs. StreamIt is a language + runtime system for stream (i.e. dataflow,
workflow, whatever) applications for GPUs. its' main point seem to be that they use dedicated threads for communication between main and local GPU memory, and between
CPU and GPU memory. anyway, they extend this language so that it can automatically handle multiple GPUs. the first step in this is partitioning the initial graph of the application. the second step is deciding what partition to map to what GPU. they also somehow (without any explanation) deduce what partitions are not suitable for GPU execution, and they map them to CPUs. this part sounds very related to our work, but they don't offer any details. the main part of the paper seems to be this partitioning algorithm. finally, they present speedup figures, which sound quite good (up to 200 compared to the sequential CPU implementation for FMRadio). examples used : discrete cosine transformation, DES, FMRadio, Matrix mult, matrix mult 3, bitonic, FFT, bitonic rec). conclusion : nice paper for citing}
}

@INPROCEEDINGS{OmpSs, 
author={Bueno, J. and Planas, J. and Duran, A. and Badia, R.M. and Martorell, X. and Ayguade, E. and Labarta, J.}, 
booktitle={Parallel Distributed Processing Symposium (IPDPS), 2012 IEEE 26th International}, 
title={{Productive Programming of GPU Clusters with OmpSs}}, 
year={2012}, 
month={may}, 
volume={}, 
number={}, 
pages={557 -568}, 
keywords={GPU clusters;OmpSs;affinity scheduling;caching;compiler;computational task;local node;overlapping communication;productive programming;remote nodes;runtime system;serial application annotation;task parallelism asynchrony;task parallelism heterogeneity;task-based parallelization;cache storage;graphics processing units;parallel programming;pattern clustering;program compilers;scheduling;}, 
doi={10.1109/IPDPS.2012.58}, 
ISSN={1530-2075},
whatisThis = {the extension of OpenMP that supports asynchronous task parallelism for clusters of GPUs. basically, they provide a few extensions to the OpenMP, and
data-transfers and scheduling is handled automatically between different GPUs. they use afinnity scheduling, caching and overlapping of communication with the computation (really, just a standard stuff). somewhat related, but lower-level than what we do, and not targeting CPUs. still, nice to cite}
}

@INPROCEEDINGS{Chapel, 
author={Sidelnik, A. and Maleki, S. and Garzaran, M. and Chamberlain, B. and Padua, D.}, 
booktitle={Parallel Distributed Processing Symposium (IPDPS), 2012 IEEE 26th International}, 
title={{Performance Portability with the Chapel Language}}, 
year={2012}, 
month={may}, 
volume={}, 
number={}, 
pages={}, 
keywords={}, 
doi={10.1109/IPDPS.2012.58}, 
ISSN={1530-2075},
whatisThis = {Chapel language for data parallelism on multicore systems and GPUs. A simple data-parallel program with which you can write short programs that translate to CUDA. Of course, the speedups of the Chapel programs are far better than the ones of hand-optimized CUDA code (i would be surprised if it was otherwise). anyway, you can have explicit or implicit data transfers, scheduling is done by runtime system (i think). blah blah. anyway, the main selling point is that the same code works both on GPUs and on multicore CPUs. but the code cannot work on GPUs AND CPUs at the same time. so, we are good. still worth citing as something vaguely related.}
}

@INPROCEEDINGS{StreamIt, 
author={Hagiescu, A. and Huynh Phung Huynh and Weng-Fai Wong and Goh, R.S.M.}, 
booktitle={Parallel Distributed Processing Symposium (IPDPS), 2011 IEEE International}, 
title={{Automated Architecture-Aware Mapping of Streaming Applications Onto GPUs}}, 
year={2011}, 
month={may}, 
volume={}, 
number={}, 
pages={467 -478}, 
keywords={AMD;automated architecture-aware mapping;graphic processing units;hardware scheduler;interleaved execution;nVidia GPUs;processing cores;streaming multiprocessors;warps;wave fronts;coprocessors;multiprocessing systems;}, 
doi={10.1109/IPDPS.2011.52}, 
ISSN={1530-2075},
whatIsThis={Original StreamIt paper. Didn't read it. Will do in the future.}
}

@inproceedings{Qilin,
 author = {Luk, Chi-Keung and Hong, Sunpyo and Kim, Hyesoon},
 title = {{Qilin: Exploiting Parallelism on Heterogeneous Multiprocessors with Adaptive Mapping}},
 booktitle = {Proceedings of the 42nd Annual IEEE/ACM International Symposium on Microarchitecture},
 series = {MICRO 42},
 year = {2009},
 isbn = {978-1-60558-798-1},
 location = {New York, New York},
 pages = {45--55},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1669112.1669121},
 doi = {10.1145/1669112.1669121},
 acmid = {1669121},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPU, adaptive, dynamic compilation, heterogeneous, mapping, multicore},
 whatIsThis = {presents the Qiling extension to C + runtime system for dynamic adaptive mapping of computations to CPUs/GPUs. uses some form of machine learning to 
derive good mapping of parts of computation to CPUs/GPUs. basically, the program being executed is characterised by input size N, which is the sole parameter (or however it is called) for the model. then they use profiling to estimate TCPU(n) and TGPU(n) to estimate the runtime of the same problem with size n on CPUs/GPUs. they do this for some sample of ns, then use curve fitting to get the functions TCPU and TGPU, and then they estimate which portion of the problem should be mapped to CPUs and what to GPUs. basically, the assumption is that problem is one dimensional and that the same computation is mapped to CPUs/GPUs. they don't consider e.g. mapping of different stages of large parallel programs to CPUs and GPUs. they only support operations on arrays, and assume that a single operation is applied to array and that array can be broken in chunks arbitrarily and the operation performed on these chunks. worth citing, but much less advanced compared to what we plan to do}
}

@INPROCEEDINGS{GreenGPU, 
author={Ma, Kai and Li, Xue and Chen, Wei and Zhang, Chi and Wang, Xiaorui}, 
booktitle={Parallel Processing (ICPP), 2012 41st International Conference on}, 
title={{GreenGPU: A Holistic Approach to Energy Efficiency in GPU-CPU Heterogeneous Architectures}}, 
year={2012}, 
month={sept.}, 
volume={}, 
number={}, 
pages={48 -57}, 
keywords={}, 
doi={10.1109/ICPP.2012.31}, 
ISSN={0190-3918},
whatIsThis = {GreenGPU for mapping computations to heterogeneous systems consisting of CPUs/GPUs and then scaling down frequencies of CPUs and GPUs. the point is energy efficiency, but probably worth citing since it involves some adaptive mapping of computations to CPUs and GPUs}
}